{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "taxi_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "given-segment"
      },
      "source": [
        "# Inspired from https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "import os\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from IPython.display import clear_output"
      ],
      "id": "given-segment",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "encouraging-surname"
      },
      "source": [
        "# Taxi-v3\n",
        "This task was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."
      ],
      "id": "encouraging-surname"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlimited-friendly"
      },
      "source": [
        "env = gym.make(\"Taxi-v3\").env"
      ],
      "id": "unlimited-friendly",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "after-gallery"
      },
      "source": [
        "![Taxi environment](images/taxi_env.png)"
      ],
      "id": "after-gallery"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duplicate-registration",
        "outputId": "b268ea74-1b63-4942-dc9e-42c52bb4a676"
      },
      "source": [
        "env.render()"
      ],
      "id": "duplicate-registration",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudden-manitoba"
      },
      "source": [
        "* **`env.reset`**: Resets the environment and returns a random initial state.\n",
        "* **`env.step(action)`**: Step the environment by one timestep. Returns\n",
        "    * **observation**: Observations of the environment\n",
        "    * **reward**: If your action was beneficial or not\n",
        "    * **done**: Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
        "    * **info**: Additional info such as performance and latency for debugging purposes\n",
        "* **`env.render`**: Renders one frame of the environment (helpful in visualizing the environment)"
      ],
      "id": "sudden-manitoba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dried-inclusion",
        "outputId": "4a7cd8c4-b946-47f8-9547-13f20797c9f5"
      },
      "source": [
        "env.reset()  # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "id": "dried-inclusion",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| :\u001b[43m \u001b[0m|B: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3xA611nmyfo",
        "outputId": "275146b0-d750-4b7a-c26a-5d97cc5150c6"
      },
      "source": [
        "env.reset()  # reset environment to a new, random state"
      ],
      "id": "i3xA611nmyfo",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "486"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "expensive-course"
      },
      "source": [
        "* 0 = south\n",
        "* 1 = north\n",
        "* 2 = east\n",
        "* 3 = west\n",
        "* 4 = pickup\n",
        "* 5 = dropoff"
      ],
      "id": "expensive-course"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "external-inventory",
        "outputId": "e0bf533b-cc9f-4992-e1d0-9e9bff3866f5"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "id": "external-inventory",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "removed-smooth",
        "outputId": "6d9cdc97-7b5f-4a2b-9b13-3b37ae044af2"
      },
      "source": [
        "env.P[328]"
      ],
      "id": "removed-smooth",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brutal-uganda"
      },
      "source": [
        "$$Q(state,action)\\leftarrow (1 - \\alpha)Q(state, action)+\\alpha(reward+\\gamma max_a Q(next state, all actions))$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $\\large\\alpha$ (alpha) is the learning rate ($0<\\alpha\\leq 1$) - Just like in supervised learning settings, Î± is the extent to which our Q-values are being updated in every iteration.\n",
        "\n",
        "- $\\large\\gamma$ (gamma) is the discount factor ($0\\leq\\gamma\\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy."
      ],
      "id": "brutal-uganda"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "technological-charge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b0b0cf-c6ee-4e24-98a9-93c25cabd73f"
      },
      "source": [
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "q_table"
      ],
      "id": "technological-charge",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chronic-things",
        "outputId": "c6cd01b0-3a6d-4109-a76e-28fee3b32df3"
      },
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "seed= 42\n",
        "rng =np.random.default_rng(seed)\n",
        "\n",
        "# Training rounds\n",
        "for i in range(1, 100001):\n",
        "    # Initialize the state of the taxi, \n",
        "    # because we want to start every training randomly\n",
        "    state = env.reset() # e.g. 464\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    # If the taxi doesn't finish the pick and drop off job.\n",
        "    while not done:\n",
        "        if rng.random() < epsilon: # Judge whether we should explore or exploit\n",
        "            # Explore action space\n",
        "            action = env.action_space.sample() \n",
        "        else:\n",
        "            # Exploit learned values;\n",
        "            # Choose the best action such as go north, go south... from the previous experience\n",
        "            action = np.argmax(q_table[state])  \n",
        "            \n",
        "        next_state, reward, done, info = env.step(action) # The result of the executed action\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        # Choose the maximum value of the [next_state] column, i.e. actions\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        # Update q_table.\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state \n",
        "        # Count the number of the movements of the taxi.\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 1000 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "id": "chronic-things",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vu5c7-m4E-K",
        "outputId": "52f01a51-7c78-47d8-fb5c-d77be01e53b2"
      },
      "source": [
        "q_table"
      ],
      "id": "7Vu5c7-m4E-K",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [ 7.44058996,  8.52582799,  7.44059037,  8.52584749,  9.6220697 ,\n",
              "        -0.47415111],\n",
              "       [11.84782409, 12.97761685, 11.84784153, 12.97760868, 14.11880599,\n",
              "         3.97761755],\n",
              "       ...,\n",
              "       [ 3.4212627 , 15.27151404, -1.09451647,  0.22077914, -1.16545104,\n",
              "        -1.28723613],\n",
              "       [ 0.53860452, 10.72931985, -2.56775851,  3.63646139, -2.62771422,\n",
              "        -2.56389512],\n",
              "       [11.4040946 ,  7.58878946, 11.80268776, 18.8       ,  2.3055668 ,\n",
              "         2.9360754 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arctic-lawyer",
        "outputId": "daa159fa-43d1-4be0-f701-7c147060e97a"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "# Initalize the total epoches and the total penalitie\n",
        "total_epochs, total_penalties = 0, 0\n",
        "# \n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes): # Single training round\n",
        "    state = env.reset()  # reset environment to a new, random state\n",
        "    # Initialize the epoch, penalty and reward\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    # Init: the taxi doens't both pick up and drop off a passenger\n",
        "    done = False\n",
        "    \n",
        "    # The taxi tries to pick up and drop off one passenger\n",
        "    while not done:\n",
        "        # According to the q_table we have worked out above,\n",
        "        # we pick up the best action based on the maximum value in q_table[state].\n",
        "        action = np.argmax(q_table[state])\n",
        "        # Update the state\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "id": "arctic-lawyer",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.89\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introductory-fashion"
      },
      "source": [
        "Hyperparameters and optimizations\n",
        "The values of ($\\large\\alpha$) `alpha`, ($\\large\\gamma$) `gamma`, and ($\\large\\epsilon$) `epsilon` were mostly based on intuition and some \"hit and trial\", but there are better ways to come up with good values.\n",
        "\n",
        "Ideally, all three should decrease over time because as the agent continues to learn, it actually builds up more resilient priors;\n",
        "\n",
        "$\\Large\\alpha$: (the learning rate) should decrease as you continue to gain a larger and larger knowledge base.\n",
        "\n",
        "$\\Large\\gamma$: as you get closer and closer to the deadline, your preference for near-term reward should increase, as you won't be around long enough to get the long-term reward, which means your gamma should decrease.\n",
        "\n",
        "$\\Large\\epsilon$: as we develop our strategy, we have less need of exploration and more exploitation to get more utility from our policy, so as trials increase, epsilon should decrease."
      ],
      "id": "introductory-fashion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rocky-initial",
        "outputId": "dcb605c4-0fb3-428a-a90f-47fb87b71324"
      },
      "source": [
        "# Play an episode\n",
        "\n",
        "state = env.reset()  # reset environment to a new, random state\n",
        "env.render()\n",
        "time.sleep(0.5)\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(q_table[state])\n",
        "    state, reward, done, info = env.step(action)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print(reward)\n",
        "    time.sleep(0.5)"
      ],
      "id": "rocky-initial",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "20\n"
          ]
        }
      ]
    }
  ]
}